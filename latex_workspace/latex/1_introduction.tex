\section{Introduction}

%here is my work
Open-Domain Question Answering (QA) is a task that aims to answer the factoid questions with amounts of documents. Previous QA systems \cite{chen2017reading} often consist of multiple components, including retriever and reader. The retriever first retrieves a small subset of the documents using the question as a query, and the reader utilize the retrieved documents as input to extract or generate the answer. 

For the purpose of enhancing the ability of the QA system, we can improve the performance of the retriever by reranking the retrieval results. Previous work \cite{karpukhin2020dense} shows that better performance in QA system can be achieved when the retrieval results are improved. Rerank the retrieval results by reranker which is a very useful approach and is wildly used after retrieval stage.

However, one limitation of the Retriever-Reranker-Reader architecture (R3) is the reranker. Conventional reranker always rerank the retrieved documents directly, it is suboptimal. Previous work  based on BERT \cite{nogueira2019passage, gao2021rethink}, rerank the retrieval documents through the questions and documents relevance. RIDER \cite{mao2021rider}  use the reader answer to rerank the documents, while it is instable.Current mainly based on seq2seq model \cite{sachan2022improving}, but is often spend much resource and time to train and inference. Therefore, we combine the answer generated by reader and the rerank model that need less resource.


%提出自己的模型
In this paper, we propose a novel Reader Help Rerank model, \system{}, which promote the QA system performance. We concatenate the question, document and answer, inputing to the \system{}, and then train the model for calculating the relevance of the question with document. At inference time, \system{} ranks the retrieved documents with the score.

We excute experiments on the Natural Questions (NQ) \cite{kwiatkowski2019natural} and TriviaQA (Trivia) \cite{joshi2017triviaqa} datasets. \system{} outperforms the state-of-the-art OpenQA systems \citep{karpukhin2020dense,sachan2021end}, and achieves EM=50 on NQ dataset and EM=51.2 on Trivia dataset. This shows the effectiveness and generalization of our approach.

Our contributions are as follows:

1. we propose \system{} to rerank the retrieved passages and achieve 10 gains in top-1 retrieval accuracy and 3 gains in QA.

2. The \system{} not only performs well on in-domain datasets, but also works well on out-of-domain datasets, showing strong generalization ability.
