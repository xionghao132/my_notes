\section{Related Work}
Open-domain Question Answering (QA for short) requires a system to answer questions based on evidence which are retrieved from a large corpus such as Wikipedia. Current approaches consist of retriever, reranker and reader networks, where the retriever retrieves a small number of documents, and the reranker reranks the retrieved documents for the reader to answer the questions. 

\paragraph{Retriever} Text retrieval aims to find related documents from a large corpus based on a query. Earlier work \cite{chen2017reading} relied on bag-of-words-based sparse retrievers such as TF-IDF \cite{chen2017reading} and BM25 \cite{Robertson2009ThePR}. Recently,
some work improve the traditional sparse retriever with neural networks, \citet{Dai_2019} use BERT \cite{devlin2019bert} to dynamically generate term weights, and \citet                                                {mao2021generationaugmented} utilize text generation method to expand queries or documents to make better use of sparse retriever.

More recent work showed that neural retrievers can generate effective dense representations for retrieval when trained on open-domain QA datasets \cite{karpukhin2020dense}. \citet{qu2021rocketqa} improve the approach further with the hard negative sampling by iterative training. \citet{izacard2022distilling} distill knowledge from reader to retriever. There also exists some researchers focus on the pre-training of dense retrieval \cite{gao2021condenser}.

\paragraph{Reranker} Previous work showed that the pretrained language models demonstrated an outstanding capability in enhancing the performance of both sparse and dense retrievers. \citet{nogueira2019passage} present a supervised reranker for retrieval tasks based on BERT to rerank the retrieved documents. \citet{mao2021rider} propose to rerank by the reader predictions without training a new model. \citet{sachan2022improving} use a large language model as the reranker directly,  compared to the previous method. Nonetheless, it requires large amounts of computation and time at training and inference stage and performs not well as fine-tuned reranker. \citet{chuang2023expand} propose a query reranker to select the best query expand for improving document retrieval.

\paragraph{Reader} Reader models for open-domain QA are required to read multiple documents which are more than 100 documents to avoid missing the target document from the large-scale knowledge base. The reader models was divided into two primary categories, the extractive readers \cite{karpukhin2020dense}, which encode each document separately and marginalize the predicted answer probabilities and extract the answer spans from the probabilities.

While the generative readers generating the answer in a sequence-to-sequence manner,including the Fusion-in-Decoder (FiD) model \cite{izacard2020leveraging} and the Retrieval-Augmented Generation model \cite{lewis2020retrieval}. The FiD model concatenate the encoded representations of documents, which can then be decoded by the decoder and generate the answer for the query. 
