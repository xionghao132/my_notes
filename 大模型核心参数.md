# 大模型核心参数

## 概述

我们熟悉的大多数语言模型都是通过重复生成token序列（sequence）中的下一个token来运作的。每次模型想要生成另一个token时，会重新阅读整个token序列并预测接下来应该出现的token。这种策略被称为**自回归生成（autoregressive generation）**。



概率分布解码方法：

- **贪心解码（Greedy Decoding）**：直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复。
- **随机采样（Random Sampling）**：按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义。
- **Beam Search**：维护一个大小为 k 的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k 个单词，然后保留总概率最高的 k 个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。



**top-k 采样和 top-p 采样是介于贪心解码和随机采样之间的方法，也是目前大模型解码策略中常用的方法。**



贪心解码是一种合理的策略，但也有一些缺点。例如，输出可能会陷入重复循环。当你不断地选择建议最高的单词时，它可能会变成重复的句子。



## Top-k

**Top-k 采样是对前面“贪心策略”的优化，它从排名前 k 的 token 中进行抽样，允许其他分数或概率较高的token 也有机会被选中。在很多情况下，这种抽样带来的随机性有助于提高生成质量。**

**【top-k 采样思路】：在每一步，只从概率最高的 k 个单词中进行随机采样，而不考虑其他低概率的单词。**



代码实现



```python
import torch
from labml_nn.sampling import Sampler

# Top-k Sampler
class TopKSampler(Sampler):
    # k is the number of tokens to pick
    # sampler is the sampler to use for the top-k tokens
    # sampler can be any sampler that takes a logits tensor as input and returns a token tensor; e.g. `TemperatureSampler`.
    def __init__(self, k: int, sampler: Sampler):
        self.k = k
        self.sampler = sampler

    # Sample from logits
    def __call__(self, logits: torch.Tensor):
        # New logits filled with −∞; i.e. zero probability
        zeros = logits.new_ones(logits.shape) * float('-inf')
        # Pick the largest k logits and their indices
        values, indices = torch.topk(logits, self.k, dim=-1)
        # Set the values of the top-k selected indices to actual logits.
        # Logits of other tokens remain −∞
        zeros.scatter_(-1, indices, values)
        # Sample from the top-k logits with the specified sampler.
        return self.sampler(zeros)

```



(1）top-k 优点：

它可以根据不同的输入文本动态调整候选单词的数量，而不是固定为 k 个。这是因为不同的输入文本可能会导致不同的概率分布，有些分布可能比较平坦，有些分布可能比较尖锐。如果分布比较平坦，那么前 k 个单词可能都有相近的概率，那么我们就可以从中进行随机采样；如果分布比较尖锐，那么前 k 个单词可能会占据绝大部分概率，那么我们就可以近似地进行贪心解码。
它可以通过调整 k 的大小来控制生成的多样性和质量。一般来说，k 越大，生成的多样性越高，但是生成的质量越低；k 越小，生成的质量越高，但是生成的多样性越低。因此，我们可以根据不同的任务和场景来选择合适的k 值。
它可以与其他解码策略结合使用，例如温度调节（Temperature Scaling）、重复惩罚（Repetition Penalty）、长度惩罚（Length Penalty）等，来进一步优化生成的效果。
（2）top-k 缺点：

它可能会导致生成的文本不符合常识或逻辑。这是因为 top-k 采样只考虑了单词的概率，而没有考虑单词之间的语义和语法关系。例如，如果输入文本是“我喜欢吃”，那么即使饺子的概率最高，也不一定是最合适的选择，因为可能用户更喜欢吃其他食物。
它可能会导致生成的文本过于简单或无聊。这是因为 top-k 采样只考虑了概率最高的 k 个单词，而没有考虑其他低概率但有意义或有创意的单词。例如，如果输入文本是“我喜欢吃”，那么即使苹果、饺子和火锅都是合理的选择，也不一定是最有趣或最惊喜的选择，因为可能用户更喜欢吃一些特别或新奇的食物。
因此，我们通常会考虑 top-k 和其它策略结合，比如 top-p。



## Top-p

top-k 有一个缺陷，那就是“k 值取多少是最优的？”非常难确定。于是出现了动态设置 token 候选列表大小策略——即核采样（Nucleus Sampling）。





【top-p 采样:思路】：在每一步，只从累积概率超过某个阈值 p 的最小单词集合中进行随机采样，而不考虑其他低概率的单词。这种方法也被称为核采样（nucleus sampling），因为它只关注概率分布的核心部分，而忽略了尾部部分。
例如，如果 p=0.9，那么我们只从累积概率达到 0.9 的最小单词集合中选择一个单词，而不考虑其他累积概率小于 0.9 的单词。这样可以避免采样到一些不合适或不相关的单词，同时也可以保留一些有趣或有创意的单词。



top-p 值通常设置为比较高的值（如0.75），目的是限制低概率 token 的长尾。我们可以同时使用 top-k 和 top-p。如果 k 和 p 同时启用，则 p 在 k 之后起作用。



代码实现

```python
import torch
from torch import nn

from labml_nn.sampling import Sampler


class NucleusSampler(Sampler):
    """
    ## Nucleus Sampler
    """
    def __init__(self, p: float, sampler: Sampler):
        """
        :param p: is the sum of probabilities of tokens to pick $p$
        :param sampler: is the sampler to use for the selected tokens
        """
        self.p = p
        self.sampler = sampler
        # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits
        self.softmax = nn.Softmax(dim=-1)

    def __call__(self, logits: torch.Tensor):
        """
        Sample from logits with Nucleus Sampling
        """

        # Get probabilities $P(x_i | x_{1:i-1})$
        probs = self.softmax(logits)

        # Sort probabilities in descending order
        sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)

        # Get the cumulative sum of probabilities in the sorted order
        cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)

        # Find the cumulative sums less than $p$.
        nucleus = cum_sum_probs < self.p

        # Prepend ones so that we add one token after the minimum number
        # of tokens with cumulative probability less that $p$.
        nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)

        # Get log probabilities and mask out the non-nucleus
        sorted_log_probs = torch.log(sorted_probs)
        sorted_log_probs[~nucleus] = float('-inf')

        # Sample from the sampler
        sampled_sorted_indexes = self.sampler(sorted_log_probs)

        # Get the actual indexes
        res = indices.gather(-1, sampled_sorted_indexes.unsqueeze(-1))

        #
        return res.squeeze(-1)

```

## Temperature采样





Temperature 采样受统计热力学的启发，高温意味着更可能遇到低能态。

在概率模型中，logits 扮演着能量的角色，我们可以通过将 logits 除以温度来实现温度采样，然后将其输入 Softmax 并获得采样概率。

temperature这个参数可以告诉机器如何在质量和多样性之间进行权衡。较低的 temperature 意味着更高的质量，而较高的 temperature 意味着更高的多样性。当 temperature 设置为零时，模型总是会选择具有最高可能性分数的token，从而导致模型生成的回复缺乏多样性，但却能确保总是选择模型评估出的最高质量的token来生成回复。



很相似，本质上就是在 Softmax 函数上添加了温度（T）这个参数。Logits 根据我们的温度值进行缩放，然后传递到 Softmax 函数以计算新的概率分布。

代码实现

```pytorch
import torch
from torch.distributions import Categorical

from labml_nn.sampling import Sampler


class TemperatureSampler(Sampler):
    """
    ## Sampler with Temperature
    """
    def __init__(self, temperature: float = 1.0):
        """
        :param temperature: is the temperature to sample with
        """
        self.temperature = temperature

    def __call__(self, logits: torch.Tensor):
        """
        Sample from logits
        """

        # Create a categorical distribution with temperature adjusted logits
        dist = Categorical(logits=logits / self.temperature)

        # Sample
        return dist.sample()

```

## 联合采样（top-k & top-p & Temperature）

通常我们是将 top-k、top-p、Temperature 联合起来使用。使用的先后顺序是 `top-k->top-p->Temperature`。

## frequency penalty 和 presence penalty

频率惩罚和存在惩罚（frequency and presence penalties）。 这些参数是另一种让模型在质量和多样性之间进行权衡的方法。temperature 参数通过在token选择（token sampling）过程中添加随机性来实现输出内容的多样性，而频率惩罚和存在惩罚则通过对已在文本中出现的token施加惩罚以增加输出内容的多样性。 这使得对旧的和过度使用的token进行选择变得不太可能，从而让模型选择更新颖的token。

**频率惩罚（frequency penalty）**：让token每次在文本中出现都受到惩罚。这可以阻止重复使用相同的token/单词/短语，同时也会使模型讨论的主题更加多样化，更频繁地更换主题。
**存在惩罚（presence penalty）**：一种固定的惩罚，如果一个token已经在文本中出现过，就会受到惩罚。这会导致模型引入更多新的token/单词/短语，从而使其讨论的主题更加多样化，话题变化更加频繁，而不会明显抑制常用词的重复。
就像 temperature 一样，频率惩罚和存在惩罚（frequency and presence penalties）会引导我们远离“最佳的”可能回复，朝着更有创意的方向前进。然而，它们不像 temperature 那样通过引入随机性，而是通过精心计算的针对性惩罚，为模型生成内容增添多样性在一些罕见的、需要非零 temperature 的任务中（需要对同一个提示语给出多个答案时），可能还需要考虑将小的频率惩罚或存在惩罚加入其中，以提高创造性。但是，对于只有一个正确答案且您希望一次性找到合理回复的提示语，当您将所有这些参数设为零时，成功的几率就会最高。

一般来说，如果只存在一个正确答案，并且您只想问一次时，就应该将频率惩罚和存在惩罚的数值设为零。但如果存在多个正确答案（比如在文本摘要中），在这些参数上就可以进行灵活处理。如果您发现模型的输出乏味、缺乏创意、内容重复或内容范围有限，谨慎地应用频率惩罚或存在惩罚可能是一种激发活力的好方法。但对于这些参数的最终建议与 temperature 的建议相同：在不确定的情况下，将它们设置为零是一个最安全的选择！




[大模型核心参数解析（Top-k、Top-p、Temperature、frequency penalty、presence penalty）_大模型top k-CSDN博客](https://blog.csdn.net/u012856866/article/details/140308083)