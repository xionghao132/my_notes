# 实验室计划

## 读论文

emnlp:[Conference on Empirical Methods in Natural Language Processing - ACL Anthology](https://aclanthology.org/venues/emnlp/)

- [x] **应该是更强的DPR  Fid-KD**   **DKRR**：Gautier Izacard and Edouard Grave. 2021a. Distilling knowledge from reader to retriever for question answering. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

[五万字综述！Prompt-Tuning：深度解读一种新的微调范式 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/618871247)

[密集段落检索器DPR的复现研究 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/599874631)  A Replication Study of Dense Passage Retriever

[QA中的信息检索技术（IR）整理_qa数据集如何做retrieval_北在哪的博客-CSDN博客](https://blog.csdn.net/qq_43183860/article/details/121381192)

[一文梳理DPR(Dense Passage Retrieval)的发展 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/559720649)



它们经常查询复杂的、非事实的信息，并且需要困难的类似蕴涵的推理来解决。


Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. 2004. Umass at trec 2004: Novelty and hard. Computer Science Department Faculty Publication Series, page 189.

Few-shot generative conversational query rewriting.



**the weakness of dense retriever**

information loss:Sparse, dense, and attentional representations for text retrieval.

dense retriever makes it hart to match entity:Simple entity-centric questions challenge dense retrievers

it is hard to transfer domain:Towards robust neural retrieval models with synthetic pre-training



- [x] LLM T0:Multitask prompted training enables zero-shot task generalization




DeBERTa V3 base:Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing

SEAL:Autoregressive search engines: Generating substrings as document identifiers.

- [x] BM25/GAR fusion:Query expansion using contextual clue sampling with language models


Learning to retrieve passages without supervision.

Passage re-ranking with bert.

5https://huggingface.co/bigscience/T0_3B 

6https://huggingface.co/microsoft/deberta-v3-base 

7https://github.com/yixinL7/SimCLS

**Efficient ODQA**

BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

GPT：Language models are few-shot learners.

use the GPT to instead the T5 or BART 

R2-D2: A modular baseline for opendomain question answering.

UnitedQA: A hybrid approach for open domain question answering.

memory-intensive:Structured pruning learns compact and accurate models.

RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering.

PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them.

LIDER:An efficient high-dimensional learned index for large-scale dense passage retrieval

APE:Training adaptive computation for open-domain question answering with computational constraints



**Reranking**

- [x] Rerank:Passage re-ranking with bert.


- [x] use the LLM to rerank：Improving passage retrieval with zero-shot question generation.


Splade: Sparse lexical and expansion model for first stage ranking.

- [x] SEAL:Autoregressive search engines: Generating substrings as document identifiers



最近可以把zotero上的论文做好标签和笔记，标签都标号年份，标签最好都是英文。

## 实现代码



- [x] `DPR`

- [x] `T5-FID`

- [x] `DRQA`

## 整理数据集

- [x] NQ


- [x] HotPot


- [x] triviaQA

目前这三个论文读完了



## 研究方向

### 检索

检索系统要研究一下索引`Index`，和数据库有关，怎么在大量数据之中进行检索。



针对检索的高效索引，有几种常见的选择。在默认设置下，可以获得高准确性，但也会使用较多的RAM。以下是一些常见的索引方法：

1. **倒排索引（Inverted Index）**：这是一种常见的文本检索方法，特别适用于大型文本集合。它将每个单词映射到包含它的文档列表，从而允许高效地根据关键字进行搜索。尽管需要一定的内存来存储索引，但它在搜索效率方面表现良好。
2. **B树和B+树**：这些树状结构被广泛用于数据库系统中的索引。B树用于磁盘上的索引，而B+树更适用于内存索引。它们允许在有序数据集上进行高效的范围查询和等值查找，并且在默认设置下可以提供较好的性能。
3. **压缩索引**：一些索引方法使用压缩算法来减少索引的存储空间占用。这可以在一定程度上减少RAM使用量，并且通常还能提供不错的检索效率。例如，一些列存储数据库系统使用了列索引的压缩变体。
4. **布隆过滤器（Bloom Filter）**：尽管不是传统意义上的索引，布隆过滤器可以用于快速判断一个元素是否存在于集合中，尤其在需要快速排除一些不可能存在的情况下。它的内存占用相对较小，但有一定的误判率。
5. **哈希索引**：哈希索引将键直接映射到存储位置，适用于等值查找。然而，在默认设置下，哈希索引可能会导致冲突，需要解决冲突的方法可能会占用更多内存。

要根据特定需求选择适当的索引方法，需要考虑数据集的规模、查询模式以及可用的资源。在默认设置下，倒排索引和B+树通常是性能较好的选择，但根据具体情况可能需要进行微调或者采用其他索引方法。



记得好像在`Elasticsearch`中接触过，也可以了解一下`Lucene`怎么检索，好像都与`TF-IDF`算法和`BM25`有关



`faiss`教程，代码部分讲解的很好:[向量检索速度慢？看看这个Faiss索引实操 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/530958094)



Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning.

- [x] **MIPS** 加速    有faiss不用管了



**ICT**：https://paperswithcode.com/paper/190600300

- [x] **Bi-encoder**:Latent retrieval for weakly supervised open domain question answering. 

`bert-ict`

Pre-training tasks for embedding-based large-scale retrieval.



- [x] **Rocketqa**: An optimized training approach to dense passage retrieval for open-domain question answering.

Using a learned retriever to mine hard negatives and re-train another retriever with them was found helpful



**Complement** lexical retrieval model with semantic residual embeddings.

proposed to jointly learn a pair of dense and sparse systems to mitigate the capacity issue with low dimension dense vectors. Beyond fine-tuning, using more sophisticated knowledge distillation loss to learn bi-encoders based on soft labels has also been found useful

- [x] **BM25** 教程

[pyserini/docs/usage-interactive-search.md at master · castorini/pyserini (github.com)](https://github.com/castorini/pyserini/blob/master/docs/usage-interactive-search.md)

- [x] RM3（Relevance Model 3）是一种基于BM25的查询扩展方法。它通过使用反馈文档的信息来改进初始查询，并重新计算文档与扩展后查询的相关性得分。


1. RM3论文：https://www.cs.cmu.edu/~callan/Papers/sigir07-rm3.pdf
2. RM3实现：直接使用`pyserini`实现

#### 代码：

- [x] `BM25`:[文本相似度（tf-idf 和 bm25的算法讲解）_bm25算法归一化_#叫啥名字呢的博客-CSDN博客](https://blog.csdn.net/weixin_40411446/article/details/80384060?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-80384060-blog-104745144.235^v38^pc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-80384060-blog-104745144.235^v38^pc_relevant_anti_vip_base&utm_relevant_index=2&ydreferer=aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1L2FydGljbGUvZGV0YWlscy8xMDQ3NDUxNDQ%2FeWRyZWZlcmVyPWFIUjBjSE02THk5amJpNWlhVzVuTG1OdmJTOCUzRA%3D%3D)

- [x] `TF-IDF` [TF-IDF 算法详解及 Python 实现_tfidf_空杯的境界的博客-CSDN博客](https://blog.csdn.net/benzhujie1245com/article/details/118228847?ydreferer=aHR0cHM6Ly93d3cuYmluZy5jb20v)

`DPR`

### 注意力机制

`self-attention:`[超详细图解Self-Attention - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/410776234)

[Self -Attention、Multi-Head Attention、Cross-Attention_cross attention_大鹏的NLP博客的博客-CSDN博客](https://blog.csdn.net/philosophyatmath/article/details/128013258?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-128013258-blog-126688503.235^v38^pc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-2-128013258-blog-126688503.235^v38^pc_relevant_anti_vip_base&utm_relevant_index=3)

`cross-attention`:[【科研】浅学Cross-attention？_cross attention_MengYa_DreamZ的博客-CSDN博客](https://blog.csdn.net/MengYa_Dream/article/details/126688503)

#### 代码：

`self-attention`

`Multi-attention`

`Transformer`

### LLM

`hugging face`:最好能熟练敲出来

`GPT2`              [基于GPT2的中文闲聊机器人/GPT2 for Chinese chitchat_chitchat csdn_红雨瓢泼的博客-CSDN博客](https://blog.csdn.net/kingsonyoung/article/details/103803067?ydreferer=aHR0cHM6Ly93d3cuYmluZy5jb20v)

[训练属于自己的ChatGPT（1）--在GPT2上进行chatbot微调实战 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/606298369)

结合KG增强：[再谈大模型x知识图谱 | 大模型领域认知进化系列 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/647611646)

开源的一个类似gpt3效果差一点，但是用到tensorflow:https://github.com/EleutherAI/gpt-neo

- [x] BART:https://blog.csdn.net/ljp1919/article/details/108816398




介绍LLM论文以及部署:https://blog.csdn.net/ljp1919/article/details/132484657

**Stanford Alpaca**：https://github.com/tatsu-lab/stanford_alpaca

**SwiGLU** ：

chatGLM：[THUDM/ChatGLM-6B: ChatGLM-6B: An Open Bilingual Dialogue Language Model | 开源双语对话语言模型 (github.com)](https://github.com/THUDM/ChatGLM-6B)

LLM的评价指标：

A survey of ChatGPT：[pengwei-iie/A_survey_and_tools_of_ChatGPT: Some survey and tools of ChatGPT or ChatGPT-Style Model (github.com)](https://github.com/pengwei-iie/A_survey_and_tools_of_ChatGPT)



- [x] 显卡量化：[NLP（十一）：大语言模型的模型量化(INT8/INT4)技术 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/627436535)




prompt:[Chatgpt实战：prompts最全合集（权威版） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/626024467)

- [x] T0：[bigscience-workshop/t-zero: Reproduce results and replicate training fo T0 (Multitask Prompted Training Enables Zero-Shot Task Generalization) (github.com)](https://github.com/bigscience-workshop/t-zero/tree/master)

llama_index:[【城南 · LlamaIndex 教程】一文看懂LlamaIndex用法，为LLMs学习私有知识 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/630832409)

### KBQA

[KBQA 常用的问答数据集之WebQuestions_webquestions数据集_Toady 元气满满的博客-CSDN博客](https://blog.csdn.net/lft_happiness/article/details/123088513?ydreferer=aHR0cHM6Ly93d3cuYmluZy5jb20v)

## 好用的资源

- [x] 生成手写体：[可爱字体大全-可爱字体库-第一字体网 (diyiziti.com)](https://www.diyiziti.com/List/keai)


prompt工具：https://zhuanlan.zhihu.com/p/628254170)

- [x] vscode修改终端：[Base16 Terminal Colors for Visual Studio Code (glitchbone.github.io)](https://glitchbone.github.io/vscode-base16-term/#/atelier-lakeside-light)


## Python库的学习

`scipy`

`nltk`

`spacy`：NER

`elasticsearch`   [elasticsearch学习一：了解 ES，版本之间的对应。安装elasticsearch，kibana，head插件、elasticsearch-ik分词器。_java冯坚持的博客-CSDN博客](https://blog.csdn.net/qq_40036754/article/details/110394095?ydreferer=aHR0cHM6Ly93d3cuYmluZy5jb20v)

- [x] `setup.py`的使用，安装库

- [x] `__init.py__`文件的作用：方便导包

[python中import包报错解决方法_一个包import另一个包中的文件会报错_damonzheng46的博客-CSDN博客](https://blog.csdn.net/a264672/article/details/123341712)

例如**特征哈希**和**随机投影**，**布隆过滤器**中也有应用。

MurmurHash3_最详细的介绍：[MurmurHash3_最详细的介绍_旧夏季 听风起的博客-CSDN博客](https://blog.csdn.net/freda1997/article/details/105199265)

```sh
python setup.py install    #稳定 
python setup.py develop    #可以动态修改
```

- [x] [Python包管理工具setuptools之setup函数参数详解](https://www.cnblogs.com/potato-chip/p/9106225.html)

使用`python`直接运行`chatgpt`：https://github.com/KillianLucas/open-interpreter



`huggingface`:要求是能自己手动敲

hugging face 不同模型模式的区别，AutoModelForSequenceClassification,AutoModel,

`wandb` 进行可视化

- [x] `pyserini` 学习  有各种检索器


[wistbean/learn_python3_spider: python爬虫教程系列、从0到1学习python爬虫，包括浏览器抓包，手机APP抓包，如 fiddler、mitmproxy，各种爬虫涉及的模块的使用，如：requests、beautifulSoup、selenium、appium、scrapy等，以及IP代理，验证码识别，Mysql，MongoDB数据库的python使用，多线程多进程爬虫的使用，css 爬虫加密逆向破解，JS爬虫逆向，分布式爬虫，爬虫项目实战实例等 (github.com)](https://github.com/wistbean/learn_python3_spider)

`tensorboard`

`comet`

学习python内存管理，这次内存不够，管理了一次成功！

## 多线程学习

- [x] 线程使用


- [x] 线程池的使用


- [x] 进程池使用



## IR

`DPR`：使用了负样本提升效果    可以结合一些预训练的方法，ICT，BFS，WLP

`ICT`:阅读理解填空去增强

`rocketQA`:挖掘数据，增强数据

`Realm`:增强的检索器去检索相关的文档



**我的想法**：对问题的每个词进行**MASK**,然后去检索相关的文档，然后去**rerank**，多于**top_k**的文章去除。数据我觉得还可以想想怎么结合负样本。



关于`rerank`可能得先去看看有没有相关论文了，或者从论文一些`rerank`的技术中得到什么灵感。

召回的模型分成两种，一种是`DPR`这种双塔结构，另一种是`Cross-attention`这种问题和文档拼接输入。



负例就是`sample softmax`

三个从检索增强上考虑的：`ORQA`，`REALM`，`RAG`

**建模过程**：$x  ->p(z|x)检索器->p(y|x,z) 生成器$

**top_k** ：` k`的选择也是个问题，基本都是固定参数进行扫描，能否有个变化的参数呢？？



**DPR**负样本构造方法：
  `Random`：从语料中随机抽取；
  `BM25`：使用BM25检索的不包含答案的段落；
  `Gold`：训练集中其他问题的答案段落
正样本构造方法：
  因为数据集如`TREC`, `WebQuestions `和` TriviaQA`中只包含问题和答案，没有段落文本，因此，论文通过`BM25`算法，在`Wikipedia`中进行检索，取`top-100`的段落，如果答案没有在里面，则丢掉这个问题。对于 `SQuAD` 和` NQ`数据集，同样使用问题在`Wikipedia`检索，如果检索到的正样本段落能够在数据集中匹配则留下，否则丢掉

**三种方法到`top100`差别不大**



**LLM**在做生成和决策方面很厉害，是否可以让他生成文档结合，或者在某一步做一个决策。

多模型生成，多模型决策。



能不能使用几个retriever的结果，比如top5 top20 top100这样的，然后使用多个reader生成结果，根据可行度来确认是否用这个模型输出，选可行度最高的。

retriever->top5 top20 top100 ->reader1 reader2 read3->max score

感觉有点像神经网络了，如果把retriever和reader都当成一个节点来看的话，那么就是说可以有权重参数。

普通神经网络的提升就是注意力机制了，交叉注意力。



**强化学习**有没有帮助(有些论文中写到这种监督的方式很费时间)

GAR使用的是PLM中的知识，没有结合外部知识。从问题入手，对问题的格式进行修改，增强。



GAR和DKRR的相同：



## 底层（最近）

> 有些内容学习不够深入，一定要能用自己的话去解释这些技术或者原理，这样自己才能衍生出其他的想法去实现。

`DPR`的模型结构，训练过程，生成`Embedding`

`BPR`怎么对`Embedding`进行编码的

`Faiss`加速过程是怎么实现的 [搜索召回 | Facebook: 亿级向量相似度检索库Faiss原理+应用 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/432317877)

我们在构建`BM25`的索引的时候，`Lucene`是怎么做的[深度解析 Lucene 轻量级全文索引实现原理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/391168762)

- [x] `BM25`-`DPR`融合过程：本质就是都取`top_k`，然后融合两个集合。
- [x] `BM25`-`DPR`融合过程

`wandb`使用



怎么用时间换空间训练。

[在K40小破卡训练50层BERT Large的宝藏trick - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/134008239)

## 论文写作Latex

- [x] 在线版本：`overleaf`


- [x] 本地要下载`tex`，学习相关语法，找找是否有类似`python`这样的库管理。


## 英语单词

靠近：close closet relate

Simultaneously to our work,

技术：technique

场景：In some sense
认为：think，denote，argue，deem，reconk，observe

冠词：article

## Linux

- [x] `nohup `(已做笔记)

- [x] `sh`命令的学习，熟练写`sh`文件代码，满足日常运行代码需求，这样每次只用运行`sh`文件，方便快捷。

重新使用`vim`，坚持使用，提升指法，提升编程效率

- [x] `chmod`权限：\[ugoa...]\[\[+-=]\[rwxX]...][,...]    chmod +用户+操作符+权限+文件

`git`分支学习

- [x] `tmux`

[Linux rm命令：删除文件或目录 (biancheng.net)](https://c.biancheng.net/view/744.html)

## 问题

谷歌云盘下载较大失败问题：[解决谷歌云盘下载一半无法下载，没有权限的问题_谷歌云盘需要访问权限_maxine_gwj的博客-CSDN博客](https://blog.csdn.net/maxine_gwj/article/details/130532865)

