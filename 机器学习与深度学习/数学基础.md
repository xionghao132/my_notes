# 数学基础

## 高等数学

### 梯度

在空间的每一个点都可以确定无限多个方向，一个多元函数在某个点也必然有无限多个方向。因此，导数在这无限多个方向导数中最大的一个（它直接反映了函数在这个点的变化率的数量级）等于多少？它是沿什么方向达到的？描述这个最大方向导数及其所沿方向的矢量，就是我们所说的梯度。

[(24条消息) 梯度_chuiyuqiong7534的博客-CSDN博客](https://blog.csdn.net/chuiyuqiong7534/article/details/101022055)

### 泰勒公式

泰勒公式，也称泰勒展开式。是用一个函数在某点的信息，描述其附近取值的公式。如果函数足够平滑，在已知函数在某一点的各阶导数值的情况下，泰勒公式可以利用这些导数值来做系数，构建一个多项式近似函数，求得在这一点的邻域中的值。

**简单来讲就是用一个多项式函数去逼近一个给定的函数(即尽量使多项式函数图像拟合给定的函数图像)，注意，逼近的时候一定是从函数图像上的某个点展开。**如果一个非常复杂函数，想求其某点的值，直接求无法实现，这时候可以使用泰勒公式去近似的求该值，这是泰勒公式的应用之一。泰勒公式在机器学习中主要应用于梯度迭代。

#### 公式定义

如果函数$f(x)$在含有$x_0$在某个开区间（a，b）内具有直到（n+1）阶导数，则对于任意（a，b）：
$$
f(x)=\frac{f(x_0)}{0!}+\frac{f^{'}(x_0)}{1!} (x-x_0)+\frac{f^{''}(x_0)}{2!} (x-x_0)^2+\cdots+\frac{f^{(n)}(x_0)}{n!} (x-x_0)^n+R_n(x)
$$
其中余项（即误差）：
$$
R_n(x)=\frac{f^{(n+1)}(\zeta )}{(n+1)!}(x-x_0)^{n+1}
$$
其中$\zeta$在$x_0$和$x$之间。叫拉格朗日余项。

#### 麦克劳林公式

麦克劳林公式是泰勒公式的特殊情况：即当$x_0=0$时的泰勒公式。所以将$x_0=0$带入泰勒公式中，就可以获得：

[Taylor公式（泰勒公式）通俗+本质详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/392808684)

### 拉格朗日

### 凹凸函数

### 牛顿法

### 余弦相似度

[(37条消息) 相似度算法之余弦相似度_牧野之歌的博客-CSDN博客_余弦相似性](https://blog.csdn.net/zz_dd_yy/article/details/51926305)

### 伽马积分

推导高斯分布的使用用到了这个积分

[伽马函数的这两个积分公式你知道吗？（量子力学考研） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/486858613)

## 线性代数

### 向量

### 精度矩阵

方差矩阵的逆

### 秩

转置后秩不变

R ( A ) ≤ min ⁡ { m , n } R(A) \leq \min\{m,n\}R(A)≤min{m,n}，A AA是m行n列矩阵
R ( k A ) = R ( A ) R(kA) = R(A)R(kA)=R(A)，k kk不等于0
R ( A ) = 0 ⟺ A = 0 R(A)=0 \Longleftrightarrow A=0R(A)=0⟺A=0
R ( A + B ) ≤ R ( A ) + R ( B ) R(A+B) \leq R(A)+R(B)R(A+B)≤R(A)+R(B)

R(AB)<=min(R(A),A(B))

### 特征值

特征矩阵是正交化

$\det(A)=\lambda_1^{n_1}\lambda_2^{n_2}...\lambda_k^{n_k}$

$tr(A)=\lambda_1^{n_1}+\lambda_2^{n_2}+...+\lambda_k^{n_k}$

因为一个矩阵的行列式等于这个矩阵所有特征值的积，当有一个特征值为0时，这个矩阵的行列式就为0。

矩阵特征值的个数等于其阶数。

[(80 条消息) 为何矩阵特征值乘积等于矩阵行列式值？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/304671751)

### 二次型

$\mathbb{R}^n$上的一个二次型是一个定义在 Rn 上的函数，表达式为 $Q(x)=x^TAx$ ，其中 A 是一个 $n×n $**对称矩阵**，矩阵 A 称为**关于二次型的矩阵（二次型矩阵）**，$Q(x)$称为二次型

[线性代数——二次型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/407010460)

### 正定与半正定矩阵

#### 定义

给定一个大小为$ n×n$ 的**实对称**矩阵 $A $，若对于任意长度为 $n $的**非零**向量$ x $，有$ x^TAx>0$ 恒成立，则矩阵 A 是一个正定矩阵。

正定矩阵=对称矩阵+特征值严格大于0。

1. 正定矩阵的行列式恒为正；
2. 实对称矩阵AA正定当且仅当AA与单位矩阵合同；
3. 两个正定矩阵的和是正定矩阵；
4. 正实数与正定矩阵的乘积是正定矩阵。

$A^T=A^{-1}$

给定一个大小为$ n×n$ 的**实对称**矩阵 $A $，若对于任意长度为 $n $的**非零**向量$ x $，有$ x^TAx\ge0$ 恒成立，则矩阵 A 是一个半正定矩阵。



半正定矩阵=对称矩阵+特征值大于等于0。

1. 半正定矩阵的行列式是非负的；
2. 两个半正定矩阵的和是半正定的；
3. 非负实数与半正定矩阵的数乘矩阵是半正定的。

有点可以类比于函数$y=ax^2$

$x^TAA^Tx>=0$，所以$AA^T$一定是半正定

[浅谈「正定矩阵」和「半正定矩阵」 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/44860862)

[(80 条消息) 正定矩阵一定是对称阵吗？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/66922790)

### 矩阵求导



[矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/263777564)

[矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/273729929)

[(44条消息) 矩阵求导、几种重要的矩阵及常用的矩阵求导公式_~青萍之末~的博客-CSDN博客_矩阵求导](https://blog.csdn.net/daaikuaichuan/article/details/80620518)

矩阵的行列式对自身矩阵求导:

以方阵$A(n\times n)$为例，矩阵的逆表达式为：$A^{-1}=\frac{A^*}{|A|}$($A^*$为伴随矩阵)
$$
\frac{\partial |A|}{A}=(A^*)'=(|A|A^{-1})=|A|(A^{-1})'
$$


### 迹

#### 定义

矩阵的迹：主对角线上所有元素之和。记作$tr(A)$，其中$A$为方阵。

**注意：方阵才有迹！**

#### 性质

1. 标量的迹等于自己。
2. 矩阵的迹等于其特征值之和。
3. 矩阵转置迹不变，不影响对角线元素。 $tr(A)=tr(A^T)$   
4. 矩阵乘法的迹满足交换律，相当于矩阵点积一样。$tr(AB)=tr(BA)$   这个经常用于证明中
5. 线性关系$tr(c_1A+c_2B)=c_1tr(A)+c_2tr(B)$。

[(55条消息) 线性代数之 矩阵的迹_RuiH.AI的博客-CSDN博客](https://blog.csdn.net/qq_41035283/article/details/121268734)

#### 求导

$$
\frac{\partial tr(AB)}{A} =\frac{\partial tr(BA)}{A}=B^T
$$

$$
\frac{\partial tr(A^TB)}{A} =\frac{\partial tr(B^TA)}{A}=B
$$

[矩阵的迹及迹的求导 - Lxk- - 博客园 (cnblogs.com)](https://www.cnblogs.com/Lxk0825/p/13987066.html)

### 马氏距离

[马氏距离(Mahalanobis Distance) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/46626607)

### 协方差矩阵和相关系数矩阵

实际计算时考虑将多个随机变量放在一个矩阵$X\in \mathbb{R}^{m\times n}$中，每一列是一个随机变量，每一行是一个样本，那么协方差矩阵的矩阵计算形式就是	$Cov=E[(X-EX)^T(Y-EY)]$



**相关系数**（Pearson product-moment correlation coefficient）是统计学家Pearson提出的用于统计两个随机变量之间线性相关程度的统计量。其定义为：两个变量之间的皮尔逊相关系数定义为两个变量的[协方差](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE)除以它们[标准差](https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E6%A0%87%E5%87%86%E5%B7%AE)的乘积。

$\rho_{X_i,X_j}=\frac{\sigma_{X_i,X_j}}{\sqrt{\sigma_{X_i,X_i}\sigma_{X_j,X_j}}}$

**数学关系**：
$$
P: \text{correlation matrix} \\ \Sigma: \text{corvariance matrix} \\ V: \text{diagnal variance matrix}\\
$$


$V=diag([\sigma_{X_1 X_1}^{2}, \sigma_{X_2 X_2}^{2},..., \sigma_{X_n X_n}^{2}])$

那么，相关矩阵 $P$和协方差矩阵 $\Sigma$的关系如下：

$V^{-1/2}\Sigma V^{-1/2}=P\\ \Sigma = V^{1/2}PV^{1/2}$

[协方差矩阵和相关系数矩阵 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/363213507)

## 概率论与数理统计

### 概率分布

主要还是指概率密度函数pdf，分布函数（累积概率密度函数）

离散型数据就是出现某个数据的概率

连续型数据主要关注的是某个区间数据的概率

 [一文秒懂概率分布 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/28309212)

### 概率

### 概率密度

#### 联合概率

#### 边缘概率

#### 条件概率

#### 联合概率-条件概率结合

### 类别分布、范畴分布、共轭分布

### 独立与相关

### 大数定律和中心极限定理

### 随机变量

### 点估计

### 最大似然估计

**注意：极大似然估计的前提一定是要假设数据总体的分布，如果不知道数据分布，是无法使用极大似然估计的**

[参数估计(二).最大似然估计 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/55791843)

讲解概率和似然函数的差别 非常清楚

[(51条消息) 概率和似然，终于懂了_csdn_LYY的博客-CSDN博客](https://blog.csdn.net/csdn_lyy/article/details/117399423)

最大后验估计相当于加入了先验概率

### 贝叶斯

[贝叶斯公式简介及示例讲解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/366454777)

### 期望方差

### 协方差

### 先验概率

### 后验概率

## 信息论

### 自信息

$$
I(x)=log\frac{1}{p(x)}=-log\,p(x)
$$

### 信息熵

相当于平均信息量
$$
H(x)=E[I(x)]=E_{x}[-log\,p(x)]=-\sum_{x\in X}p(x)\,log\,p(x)
$$

### 交叉熵

[损失函数：交叉熵详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/115277553)

[一文搞懂熵(Entropy),交叉熵(Cross-Entropy) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/149186719)
$$
H(p,q)=E_{p}[-logq(x)]=-\sum_{x}p(x)\,log\,q(x)
$$



所以我们自己去实现交叉熵的时候，需要对标签进行编码，例如one-hot编码，然后再与计算出来的结果算交叉熵。

### 条件熵

[信息论（3）——联合熵，条件熵，熵的性质 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/36385989)

在得知某一确定信息的基础上获取另外一个信息时所获得的信息量。
$$
H(X,Y)=H(X)+H(Y|X)
$$

$$
H(Y|X)=-\sum_{x \in \mathcal{X}}^{} \sum_{y \in \mathcal{Y}}^{}p(x,y)\log p(y|x)
$$



### KL散度

表示两个概率分布之间的差异   编码角度 预计有多少信息损失

KL>=0 当且

这个地方的KL散度也讲解了前面的内容：[(51条消息) 机器学习：KL散度详解_Re:coder的博客-CSDN博客_kl散度](https://blog.csdn.net/Poyunji/article/details/123771660)
$$
D_{KL}(p||q)=H(p,q)-H(q)=\sum_{x}p(x)\,log\frac{p(x)}{q(x)}=E_{x\sim p(x)}[log\frac{p(x)}{q(x)}]
$$
通过后面提到的**Jensen**不等式和**凸函数**可以证明：$D_{KL}(P||Q)>=0$，当且仅当$P=Q$时。

### JS散度

将KL散度视为距离度量可能很诱人，但是我们不能使用KL散度来测量两个分布之间的距离。这是因为KL散度不是对称的。

[(61条消息) 理解JS散度(Jensen–Shannon divergence)_InceptionZ的博客-CSDN博客_js散度](https://blog.csdn.net/weixin_44441131/article/details/105878383)
$$
D_{JS}(p||q)=\frac{1}{2}D_{KL}(p||\frac{p+q}{2})+\frac{1}{2}D_{KL}(q||\frac{p+q}{2})
$$


### TF-IDF

 **TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）**是一种用于[信息检索](https://so.csdn.net/so/search?q=信息检索&spm=1001.2101.3001.7020)（information retrieval）与文本挖掘（text mining）的常用**加权技术**。

**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。**

[(56条消息) TF-IDF算法介绍及实现_Asia-Lee的博客-CSDN博客_tf-idf](https://blog.csdn.net/asialee_bird/article/details/81486700)

## 数学优化

### 凸函数

**convex** 下凸 与同济大学标注出来的不一致

设函数$f(x)$在区间$I$上连续，如果对于$I$上任意两点$x_1$，$x_2$恒有
$$
tf(x_1)+(1-t)f(x_2)>=f(tx_1+(1-t)x_2)
$$
则称函数$f(x)$在$I$上是凸的，这里的$t$是参数$(0\le t \le 1)$。

### KKT条件

[Karush-Kuhn-Tucker (KKT)条件 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/38163970)

[(62条消息) 【数学基础】KKT条件_zhaosarsa的博客-CSDN博客_kkt条件](https://blog.csdn.net/qq_32742009/article/details/81411151)

[支持向量机：Duality (pluskid.org)](https://blog.pluskid.org/archives/702)

### Jensen不等式

对于一个凸函数 $f(x)$ ，都有**函数值的期望大于等于期望的函数值**：
$$
E[f(x)]≥f(E[x])
$$
对于连续变量$x$，其对应的概率密度为$\lambda (x)$，则有
$$
\int f(x)\lambda(x)dx\ge f(\int x\lambda(x)dx)
$$

### 坐标上升法

### 梯度下降法
