# 检索相关度

## TF-IDF



### 概述

**TF-IDF(Term Frequency-Inverse Document Frequency, 词频-逆文本频率)**。`TF`指词频，`IDF`指的是逆文本频率。`TF-IDF`是一种用于信息检索与数据挖掘的常用加权技术，可以评估一个词在一个文件集或者一个语料库中对某个文件的重要程度。一个词语在一篇文章中出现的次数越多，同时在所有文章中出现的次数越少，越能够代表该文章的中心意思，**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降**。



### TF

**词频 （Term Frequency）**表示词条（关键字）在文本中出现的频率。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。

**公式：**
$$
tf_{i,j}=\frac{n_{i,j}}{ {\textstyle \sum_{k}n_{k,j}} }
$$
**即：**
$$
TF_{w}=\frac{在某一类中词条w出现的次数}{该类中所有词条数目}
$$
 其中 $n_{i,j}$ 是该词在文件 $d_j$中出现的次数，分母则是文档 $d_j$ 中所有词汇出现的次数总和。



### IDF

**逆向文件频率 （Inverse Document Frequency）**，某一特定词语的IDF，可以**由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到**，主要思想是在语料库中包含某个词条的文档越少，该词条`IDF`越大，说明这个词就有很强的类别区分能力。

**公式：**
$$
idf_i=log\frac{\left | D \right | }{\left |\{j:t_i\in d_j\} \right |}
$$
 其中，$|D|$是语料库中文件总数，$ |{j:t_i∈d_j}|$ 表示包含词语 ti 的文件数目**（即 $n_{i,j}≠0$ 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此**一般情况下使用 $1+|{j:t_i∈d_j}|$，也就是平滑

**所以公式变成：**
$$
IDF_W=log(\frac{语料中文档总数}{包含该词条W的文档数+1})
$$


### TF-IDF

$$
TF-IDF=TF*IDF
$$



即某一个文件中高频出现的词条，以及该词条在整个语料库文件中低频出现的现象，可以产生出高权重的`TF-IDF`。因此，`TF-IDF`倾向于过滤掉常见的词语，保留重要的词语。即：字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

[TF-IDF算法详解及sklearn代码实现_tfidf sklearn_智商25的憨憨的博客-CSDN博客](https://blog.csdn.net/gxc19971128/article/details/106544303?ydreferer=aHR0cHM6Ly93d3cuYmluZy5jb20v)



### 代码

```python
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer

x_train = ['TF-IDF 主要 思想 是','算法 一个 重要 特点 可以 脱离 语料库 背景',
           '如果 一个 网页 被 很多 其他 网页 链接 说明 网页 重要']

#将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频
vectorizer=CountVectorizer(max_features=10)  

#计算每一个词语的TF-IDF权值
tf_idf_transformer=TfidfTransformer()

X=vectorizer.fit_transform(x_train)
tf_idf=tf_idf_transformer.fit_transform(X)

X_train_weight=tf_idf.toarray()

print(X_train_weight)
```

* 中文设置停用词

```python
stpwrdpath ="your stop words path"  #记得用得比较多的是哈工大的
with open(stpwrdpath, 'rb') as fp:
    stopword = fp.read().decode('utf-8')  # 提用词提取
#将停用词表转换为list  
stpwrdlst = stopword.splitlines()
#如果是英文的话可以直接设置stop_words=’english’
count_vec=CountVectorizer(stop_words=stpwrdlst) #创建词袋数据结构
```

* `jieba`实现`tf-idf`

```python
import jieba.analyse
 
text='关键词是能够表达文档中心内容的词语，常用于计算机系统标引论文内容特征、信息检索、系统汇集以供读者检阅。关键词提取是文本挖掘领域的一个分支，是文本检索、文档比较、摘要生成、文档分类和聚类等文本挖掘研究的基础性工作'
 
keywords=jieba.analyse.extract_tags(text, topK=5, withWeight=False, allowPOS=())
print(keywords)
```

* 检索

```python
import numpy as np
from collections import Counter
import itertools
docs = [
    "it is a good day, I like to stay here",
    "I am happy to be here",
    "I am bob",
    "it is sunny today",
    "I have a party today",
    "it is a dog and that is a cat",
    "there are dog and cat on the tree",
    "I study hard this morning",
    "today is a good day",
    "tomorrow will be a good day",
    "I like coffee, I like book and I like apple",
    "I do not like it",
    "I am kitty, I like bob",
    "I do not care who like bob, but I like kitty",
    "It is coffee time, bring your cup",
]

docs_words = [d.replace(",", "").split(" ") for d in docs]
vocab = set(itertools.chain(*docs_words))
v2i = {v: i for i, v in enumerate(vocab)}
i2v = {i: v for v, i in v2i.items()}


def safe_log(x):
    mask = x != 0
    x[mask] = np.log(x[mask])
    return x


tf_methods = {
        "log": lambda x: np.log(1+x),
        "augmented": lambda x: 0.5 + 0.5 * x / np.max(x, axis=1, keepdims=True),
        "boolean": lambda x: np.minimum(x, 1),
        "log_avg": lambda x: (1 + safe_log(x)) / (1 + safe_log(np.mean(x, axis=1, keepdims=True))),
    }
idf_methods = {
        "log": lambda x: 1 + np.log(len(docs) / (x+1)),
        "prob": lambda x: np.maximum(0, np.log((len(docs) - x) / (x+1))),
        "len_norm": lambda x: x / (np.sum(np.square(x))+1),
    }


def get_tf(method="log"):
    # term frequency: how frequent a word appears in a doc
    _tf = np.zeros((len(vocab), len(docs)), dtype=np.float64)    # [n_vocab, n_doc]
    for i, d in enumerate(docs_words):
        counter = Counter(d)
        for v in counter.keys():   #tf不是除以该文档的长度吗
            _tf[v2i[v], i] = counter[v] / counter.most_common(1)[0][1]   #most_common（n） 筛选出前n个  归一化词频

    weighted_tf = tf_methods.get(method, None)  #这个地方为什么加了log
    if weighted_tf is None:
        raise ValueError
    return weighted_tf(_tf)


def get_idf(method="log"):
    # inverse document frequency: low idf for a word appears in more docs, mean less important
    df = np.zeros((len(i2v), 1))
    for i in range(len(i2v)):
        d_count = 0
        for d in docs_words:
            d_count += 1 if i2v[i] in d else 0
        df[i, 0] = d_count

    idf_fn = idf_methods.get(method, None)
    if idf_fn is None:
        raise ValueError
    return idf_fn(df)


def cosine_similarity(q, _tf_idf):
    unit_q = q / np.sqrt(np.sum(np.square(q), axis=0, keepdims=True))
    unit_ds = _tf_idf / np.sqrt(np.sum(np.square(_tf_idf), axis=0, keepdims=True))
    similarity = unit_ds.T.dot(unit_q).ravel()  #ravel 拉成一维数组
    return similarity


def docs_score(q, len_norm=False):
    '''
    相当于将问题当做一个doc 加入doc中，然后计算doc之间的相关度，但是注意不是正真改变，而是使用副本去改变
    '''
    q_words = q.replace(",", "").split(" ")

    # add unknown words
    unknown_v = 0
    for v in set(q_words):
        if v not in v2i:
            v2i[v] = len(v2i)
            i2v[len(v2i)-1] = v
            unknown_v += 1
    if unknown_v > 0:
        _idf = np.concatenate((idf, np.zeros((unknown_v, 1), dtype=np.float)), axis=0)
        _tf_idf = np.concatenate((tf_idf, np.zeros((unknown_v, tf_idf.shape[1]), dtype=np.float)), axis=0)
    else:
        _idf, _tf_idf = idf, tf_idf
    counter = Counter(q_words)
    q_tf = np.zeros((len(_idf), 1), dtype=np.float)     # [n_vocab, 1]
    for v in counter.keys():
        q_tf[v2i[v], 0] = counter[v]

    q_vec = q_tf * _idf            # [n_vocab, 1]

    q_scores = cosine_similarity(q_vec, _tf_idf)
    if len_norm:
        len_docs = [len(d) for d in docs_words]
        q_scores = q_scores / np.array(len_docs)
    return q_scores


def get_keywords(n=2):
    for c in range(3):
        col = tf_idf[:, c]
        idx = np.argsort(col)[-n:]
        print("doc{}, top{} keywords {}".format(c, n, [i2v[i] for i in idx]))


tf = get_tf()           # [n_vocab, n_doc]
idf = get_idf()         # [n_vocab, 1]
tf_idf = tf * idf       # [n_vocab, n_doc] 广播机制
print("tf shape(vecb in each docs): ", tf.shape)
print("\ntf samples:\n", tf[:2])
print("\nidf shape(vecb in all docs): ", idf.shape)
print("\nidf samples:\n", idf[:2])
print("\ntf_idf shape: ", tf_idf.shape)
print("\ntf_idf sample:\n", tf_idf[:2])


# test
get_keywords()
q = "I get a coffee cup"
scores = docs_score(q)
d_ids = scores.argsort()[-3:][::-1]python
print("\ntop 3 docs for '{}':\n{}".format(q, [docs[i] for i in d_ids]))
```

[基于TF-IDF的简单搜索引擎的实现_tfidf 搜索_personal_CSDN_的博客-CSDN博客](https://blog.csdn.net/personal_CSDN_/article/details/118541953)

## **BM25**



### 概述

`BM25`算法是一种计算句子与文档相关性的算法，它的原理十分简单：将输入的句子`sentence`进行分词，然后分别计算句子中每个词`word`与文档`doc`的相关度，然后进行加权求和。得到句子与文档的相关度评分。

评分公式如下：
$$
Score(Q,d)=\sum_{i}^{n}{W_iR(q_i,d)}
$$
上面公式中$W_i$表示权重，也就是$idf$值。$R(q_i,d)$是$word$   $q$与文档$d$的相关性得分。



### IDF

逆文档频率，计算公式如下：
$$
IDF(q_i)=log(\frac{N-n(q_i)+0.5}{n(q_i)+0.5})
$$
`N`表示所有文档`D`中的文档d的数目，也就是总共有多少篇文档来与`sentence`计算相关性得分，`n(q_i)`为文档`d`中包含了词`q_i`的数目。从`idf`的公式我们可以看出，`n(q_i)`越大则分母越大，分子越小，也就是相应地`IDF`值越小。这是因为加入一个词在多篇文档中出现，那么一定程度上能说明这个词应该是一个使用比较普遍的词，在任何`sentence`中他都存在，不能体现`sentence`这一句话的特殊性，因此赋予它更小的`idf`值。

代入到`BM25`算法中`idf`值作为权重，也就是说明一个词`word`在越多的文档`d`中出现，那么他与文档d计算的相关性得分就应该赋予更小的权重。



==这个地方`n(q_i)`应该表示出现几个文档里面吧？？？==

### R关性得分

先来看看$R(q_i,d)$相关性得分的一般性公式：
$$
R(q_i,d)=\frac{f_i*(k_1+1)}{f_i+K} *\frac{qf_i*(k_2+1)}{qf_i+k_2}
$$

$$
K=k_1*(1-b+b*\frac{dl}{avg(dl)})
$$

上述公式中，$k_1$,$k_2$,$b$是调节因子，一般根据经验来自己设置，通常$k1=2,b=0.75$，$f_i$表示$q_i$在文档$d$中出现的频率，$qf_i$为$q_i$在输入句子$sentence$中的频率。$dl$为文档$d$的长度，$avgdl$为文档$D$中所有文档的平均长度。

又因为在绝大部分情况下，$q_i$在$sentence$中只出现一次，即所有的$q_i$的$qf_i$基本上都是一样地，因此可以将$k_2$取$0$，然后将上述公式进行简化。
接下来就是需要计算$K$，便能计算出相关性得分$R$了。

从$K$的公式可以看出，参数$b$是调整文档长度对于相关性的影响。可以看出$b$越大，文档长度对相关性得分的影响越大，反之越小。而文档$d$的相对长度越长，$K$越大，也就是$R$的分母越大，$R$相关性得分也就越小。这里可以理解成，当文档较长时，包含的词语也就越多，那么相应地包含有$q_i$这个词的可能性也就越大，但是虽然可能性要大一点，要是当$q_i$的频率$f_i$同等的情况下，长文档与$q_i$的相关性就应该比短文档与$q_i$的相关性弱。

最后可以将**BM25**算法的相关性得分的公式进行汇总：

$$
R(q_i,d)=\sum{IDF(q_i)}*\frac{f_i*(k_1+1)}{f_i+K}*\frac{qf_i*(k_2+1)}{qf_i+k_2}
$$


### 代码

```python
import math
import jieba
import numpy as np
import logging
import pandas as pd
from collections import Counter
jieba.setLogLevel(logging.INFO)

# 测试文本
text = '''
自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
自然语言处理是一门融语言学、计算机科学、数学于一体的科学。因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，所以它与语言学的研究有着密切的联系，但又有重要的区别。
自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。因而它是计算机科学的一部分。
'''

class BM25(object):
    def __init__(self,docs):
        self.docs = docs   # 传入的docs要求是已经分好词的list
        self.doc_num = len(docs) # 文档数
        self.vocab = set([word for doc in self.docs for word in doc]) # 文档中所包含的所有词语
        self.avgdl = sum([len(doc) + 0.0 for doc in docs]) / self.doc_num # 所有文档的平均长度
        self.k1 = 1.5
        self.b = 0.75
        self.k2=0

    def idf(self,word):
        if word not in self.vocab:
            word_idf = 0
        else:
            qn = {}
            for doc in self.docs:
                if word in doc:
                    if word in qn:
                        qn[word] += 1
                    else:
                        qn[word] = 1
                else:
                    continue
            word_idf = np.log((self.doc_num - qn[word] + 0.5) / (qn[word] + 0.5))
        return word_idf

    def score(self,word):
        score_list = []
        for index,doc in enumerate(self.docs):
            word_count = Counter(doc)
            q_count=Counter(self.query)
            if word in word_count.keys():
                f = (word_count[word]+0.0) / len(doc)
            else:
                f = 0.0
          #计算qfi 每个词在问题中出现的次数
            R2=1
            if self.k2!=0:   #k2=0 R2=1
                qfi=(q_count[word]+0.0) / len(self.query)
                R2=qfi*(self.k2+1)/(qfi+self.k2)
            r_score = (f*(self.k1+1)) / (f+self.k1*(1-self.b+self.b*len(doc)/self.avgdl)) *R2
            score_list.append(self.idf(word) * r_score)
        return score_list

    def score_all(self,sequence):
        self.query=sentence   #在这里初始化问题
        sum_score = []
        for word in sequence:
            sum_score.append(self.score(word))
        sim = np.sum(sum_score,axis=0)
        return sim

if __name__ == "__main__":
  # 获取停用词
    stopwords = open('../data/hit_stopwords.txt',encoding='utf8').read().split('\n')
    doc_list = [doc for doc in text.split('\n') if doc != '']
    docs = []
    for sentence in doc_list:
        sentence_words = jieba.lcut(sentence)
        tokens = []
        for word in sentence_words:
          if word in stopwords:
            continue
          else:
            tokens.append(word)
        docs.append(tokens)


    bm = BM25(docs)
    query=['自然语言', '计算机科学', '领域', '人工智能', '领域']
    score = bm.score_all(query)
    print(score)

    query=[ '计算机科学', '领域', '人工智能', '领域']
    score = bm.score_all(query)

    print(score)
```

[史上最小白之BM25详解与实现_bm25算法_Stink1995的博客-CSDN博客](https://blog.csdn.net/Tink1995/article/details/104745144)

[Okapi BM25 - Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25)

human chat dataset



[alexa/Topical-Chat: A dataset containing human-human knowledge-grounded open-domain conversations. (github.com)](https://github.com/alexa/Topical-Chat)





[ymcui/Chinese-LLaMA-Alpaca-2: 中文LLaMA-2 & Alpaca-2大模型二期项目 + 16K超长上下文模型 (Chinese LLaMA-2 & Alpaca-2 LLMs, including 16K long context models) (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)



[Getting Started with Baselines | Efficient Open-Domain Question Answering (efficientqa.github.io)](https://efficientqa.github.io/getting_started.html)









