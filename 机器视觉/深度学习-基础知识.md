# 深度学习-基础知识

## 激活函数

[一文搞懂激活函数(Sigmoid/ReLU/LeakyReLU/PReLU/ELU) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/172254089)

谈到了一些联系，与发展。

## BN

[什么是批标准化 (Batch Normalization) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/24810318)

标准化数据的优势。

> Why does batch normalization work?
>
> (1) We know that normalizing input features can speed up learning, one intuition is that doing same thing for hidden layers should also work.
>
> (2)solve the problem of covariance shift
>
> Suppose you have trained your cat-recognizing network use black cat, but evaluate on colored cats, you will see data distribution changing(called covariance shift). Even there exist a true boundary separate cat and non-cat, you can't expect learn that boundary only with black cat. So you may need to retrain the network.
>
> For a neural network, suppose input distribution is constant, so output distribution of a certain hidden layer should have been constant. But as the weights of that layer and previous layers changing in the training phase, the output distribution will change, this cause covariance shift from the perspective of layer after it. Just like cat-recognizing network, the following need to re-train. To recover this problem, we use batch normal to force a zero-mean and one-variance distribution. It allow layer after it to learn independently from previous layers, and more concentrate on its own task, and so as to speed up the training process.
>
> (3)Batch normal as regularization(slightly)
>
> In batch normal, mean and variance is computed on mini-batch, which consist not too much samples. So the mean and variance contains noise. Just like dropout, it adds some noise to hidden layer's activation(dropout randomly multiply activation by 0 or 1).
>
> This is an extra and slight effect, don't rely on it as a regularizer.

## CycleGan

[CycleGAN详细解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/402819206)



[四天搞懂生成对抗网络（四）——CycleGAN的绝妙设计：双向循环生成的结构 - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1750730)

有代码。



[四天搞懂生成对抗网络（一）——通俗理解经典GAN - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/307527293)

## 信息论

[损失函数：交叉熵详解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/115277553)

### 熵

编码长度：
$$
L(x)=\log_{2}\frac{1}{p(x)} 
$$
香农熵：
$$
H(p)=\sum_{x}p(x)\times L(x)=\sum_{x}p(x)\times \log_{2}\frac{1}{p(x)}=-\sum_{x}p(x)\times\log_{2}p(x)
$$

### 交叉熵

把来自一个分布q的消息使用另一个分布p的最佳代码传达的平均消息长度，称为交叉熵。（相当于替换长度）

 形式上，我们可以将交叉熵定义为：
$$
H_{p} (q)= \sum_{x} q(x)\log_2{\frac{1}{p(x)}}=-\sum_{x}q(x)\log_2{p(x)}
$$
交叉熵为我们提供了一种表达两种概率分布的差异的方法。p和q的分布越不相同，p相对于q的交叉熵将越大于p的熵。

==注意：==交叉熵**不是对称的**。

### KL散度

p相对于q的散度可以定义为：
$$
D_{q}(p)=H_{q}(p)-H(p)
$$
KL散度的真正妙处在于它就像两个分布之间的距离，即KL散度可以衡量它们有多不同！

